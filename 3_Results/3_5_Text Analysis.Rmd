---
title: "3_5_Text Analysis"
output: github_document
editor_options: 
  chunk_output_type: console
---
<details>
  <summary>Click to expand!</summary>
# Libraries

```{r}
loadPkg = function(toLoad){
  for(lib in toLoad){
    if(! lib %in% installed.packages()[,1])
    { install.packages(lib, repos='http://cran.rstudio.com/') }
    suppressMessages( library(lib, character.only=TRUE) ) }
}
packs=c('tidyverse', 'tidytext', 'textdata', 'egg')
loadPkg(packs)
library(readr)
setwd("C:/Users/Matt Flaherty/Documents/Projects/eda20-team4-project")
ds_jobs <- read_csv("Data Cleaning/ds_jobs.csv")
```
</details>
## Word count

I want to use text analysis on the job description to see if there is a skill that is repeated throughout. This would be an important skill if it were repeated that applicants should attempt to improve if they are interested in the jobs.

<details>
  <summary>Click to expand!</summary>
  
1. Tokenize your corpus and generate a word count.
```{r}
job_words <- ds_jobs %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)

job_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Word Frequency for All Jobs in the Data")
```

## Interpretation

No skills were mentioned frequently so I will use TF-IDF just to see what the results yield. Keep in mind that this is for all of the jobs in the data set. I will filter by DS jobs later in the report.

## Usage

This text analysis should not be included in our final report because we have been emphasizing looking at 6 DS categories and this incorporates all job categories.

<details>
  <summary>Click to expand</summary>
# TF-IDF

1. Generate a tf-idf measure of words in your dataset.
```{r}
idf_words <- ds_jobs %>% select(job_category, job_desc) %>% 
  unnest_tokens(word,job_desc) %>% count(job_category, word, sort = T)

better_idf_words <- idf_words %>% anti_join(stop_words)

description_length <- better_idf_words %>% group_by(job_category) %>% summarize(total = sum(n()))

better_idf_words <- left_join(better_idf_words, description_length)

tfidf_words <- better_idf_words %>% bind_tf_idf(word, job_category, n)

tfidf_words <- tfidf_words %>% arrange(desc(tf_idf)) %>% slice(1:15)

tfidf_words %>% arrange(desc(tf_idf)) %>% head()
```

2. Create a visualization of the tf-idf measure and interpret your results.
```{r}
tfidf_words$word <- factor(tfidf_words$word, levels = tfidf_words$word[order(desc(tfidf_words$tf_idf))])
```

```{r}
ggplot(tfidf_words, aes(x = word, y = tf_idf))+
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```
</details>
# Word Frequency by Job Category

Word count for every job category would not be beneficial to the final report so I will begin to do text analysis on each job category and then on all of the categories combined. I am doing this to see if there are any skills that applicants should have experience with before applying. This could help them become more successful applicants.

## Data Scientist

<details>
  <summary>Click to expand</summary>
1. Tokenize your corpus and generate a word count.
```{r}
ds_words <- ds_jobs%>%
  filter(job_category == "Data Scientist")

job_words <- ds_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Data Scientist Category")
```

### Interpretation

After filtering by "Data Scientist," it looks like employees should have some kind of analytical skills as well as machine learning skills. "Models" is also in the top 15 so this could augment the machine learning skill that data scientists need. 

## Data Analyst

Now I'll check to see if there are any skills for data analyst

<details>
  <summary>Click to expand!</summary>
1. Tokenize your corpus and generate a word count.
```{r}
analyst_words <- ds_jobs%>%
  filter(job_category == "Data Analyst")

job_words <- analyst_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Data Analyst Category")
```

### Interpretation

We don't learn as much from this graph as we did for the data scientist graph. However, we see soft skills such as "team", "analysis", "research" so the employee should be equipped with the ability to work with others and look at previous data to draw conclusions.

## Data Engineer

<details>
  <summary>Click to expand!</summary>
1. Tokenize your corpus and generate a word count.
```{r}
engineer_words <- ds_jobs%>%
  filter(job_category == "Data Engineer")

job_words <- engineer_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Data Engineer Category")
```

### Interpretation

Python is listed as a commonly used word for data engineering so a data engineer ought to learn Python if they wish to be a successful applicant. Cloud is also a commonly used word so I assume this would be some type of google storage.

## Machine Learning

<details>
  <summary>Click to expand!</summary>
1. Tokenize your corpus and generate a word count.
```{r}
ml_words <- ds_jobs%>%
  filter(job_category == "Machine Learning Engineer")

job_words <- ml_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Machine Learning Engineer\nCategory")
```

### Interpretation

No hard skills are commonly used in the descriptions. Commonly used words such as "team" and "decisions" could suggest that a machine learning engineer must work with other to make decisions. This makes sense because the machine learning engineer will be making the predictive models for the company.

## Statistics

<details>
  <summary>Click to expand!</summary>
1. Tokenize your corpus and generate a word count.
```{r}
stats_words <- ds_jobs%>%
  filter(job_category == "Statistician")

job_words <- stats_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Statistician Category")
```

### Interpretation

No hard skills in the job descriptions. It would be difficult to determine much from commonly used words in the job description.

## Other Analyst

<details>
  <summary>Click to expand!</summary>
1. Tokenize your corpus and generate a word count.
```{r}
other_analyst_words <- ds_jobs%>%
  filter(job_category == "Other Analyst")

job_words <- other_analyst_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Other Analyst Category")
```

### Interpretation

Nothing to learn from. Only 28 jobs in this category so not much to choose from. There are soft skills that were seen in other data science positions such as analytical skills.

# All DS jobs
<details>
  <summary>Click to expand!</summary>
```{r}
ds_filter <- ds_jobs %>%
  filter(!is.na(job_category)) %>%
  filter(job_category == "Data Analyst" | job_category == "Data Engineer" | job_category == "Data Scientist" | job_category == "Machine Learning Engineer" | job_category == "Other Analyst" | job_category == "Statistician")

job_words <- ds_filter %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
```

```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```
</details>
```{r}
one_word <- better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words for all of the\nData Science Related Positions")

one_word
```

## Interpretation

This graph is all inclusive for the data science related positions. It shows that applicants who are interested in a data science related position should have an analytical mindset. They also should be able to work well in teams.

# Two Word Text Analysis

Let's see if we can get better results using two-word phrases for our text analysis.

tokenize text

```{r}
job_words <- ds_filter %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc,token = "ngrams",format = "text", n =2)
```

Remove stop words

```{r}
better_line_words <- job_words %>% anti_join(stop_words)
head(better_line_words[,2])

# split words b/c two word phrases aren't in stop words
best <- separate(better_line_words,col = word, into = c("first", "second"), sep = " ")

# now take out stop words from the two created columns
best_line_words <- best %>% anti_join(stop_words, by = c("first"="word"))
better_line_words <- best_line_words %>% anti_join(stop_words, by = c("second"="word"))

# join the columns back together
better_line_words$word <- paste(better_line_words$first,better_line_words$second, sep = " ")
```

Create visualization

```{r}
two_word <- better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words for all of the\nData Science Related Positions\n(2 words)")

two_word
```

## Interpretation

We did not find anything by doing a text analysis with the the most used words so we are now doing a text analysis with the most commonly used two-word phrases. Now we can see popular jobs within our data set such as machin learning and data scientist.

# Combine Plots

```{r}
ggarrange(one_word,two_word,ncol = 2, nrow = 1)
```

# Conclusions

Going into this exploration, I was interested in finding out if there was a common word among job descriptions such as a hard skill that applicants would need to learn (R, Python) that would help them get a data science related job. I found that this was not case. There were soft skills that applicants should look to hone before applying such as analytical skills and getting experience working in teams.

# Text Analysis Graphs for final report

I am indifferent on including all of the individual job category graphs. I think it would be beneficial to show our thought process through the text analysis. However, That would be 6 separate graphs and the all inclusive graph on top of our graphs for other analyses. A lot. We can still talk about our thought process through all 6 graphs even if we don't include them.