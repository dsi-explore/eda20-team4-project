---
title: "Text Analysis"
output: github_document
---

```{r, include = FALSE}
loadPkg = function(toLoad){
  for(lib in toLoad){
    if(! lib %in% installed.packages()[,1])
    { install.packages(lib, repos='http://cran.rstudio.com/') }
    suppressMessages( library(lib, character.only=TRUE) ) }
}
packs=c('tidyverse', 'tidytext', 'textdata', 'egg')
loadPkg(packs)
library(readr)

ds_jobs <- read.csv("../2_Data/ds_jobs.csv")
knitr::opts_chunk$set(message = FALSE, waring = FALSE)
```

In the file we perform text analysis on the job description to see if there are any skills or experience a candidate should focus on during their job search.

## Word count

We want to use text analysis on the job description to see if there is a skill that is repeated throughout. This would be an important skill if it were repeated that applicants should attempt to improve if they are interested in the jobs.
  
Forst, we tokenized the corpus and generated a word count.
```{r}
job_words <- ds_jobs %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)

job_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

Second, we removed stop words and generated a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

Lastly, we created a visualization of the word count distribution. 
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Word Frequency for All Jobs in the Data")
```

## Interpretation

No skills were mentioned frequently so we wanted to use TF-IDF just to see what the results yield. Keep in mind that this is for all of the jobs in the data set. We will filter by DS jobs later in the report.

## Usage

This text analysis is not included in our final report because we have been emphasizing looking at 6 DS categories and this incorporates all job categories.

# TF-IDF

First we generated a tf-idf measure of words in the dataset.
```{r}
idf_words <- ds_jobs %>% select(job_category, job_desc) %>% 
  unnest_tokens(word,job_desc) %>% count(job_category, word, sort = T)

better_idf_words <- idf_words %>% anti_join(stop_words)

description_length <- better_idf_words %>% group_by(job_category) %>% summarize(total = sum(n()))

better_idf_words <- left_join(better_idf_words, description_length)

tfidf_words <- better_idf_words %>% bind_tf_idf(word, job_category, n)

tfidf_words <- tfidf_words %>% arrange(desc(tf_idf)) %>% slice(1:15)

tfidf_words %>% arrange(desc(tf_idf)) %>% head()
```

Second, we created a visualization of the tf-idf measure.
```{r}
tfidf_words$word <- factor(tfidf_words$word, levels = tfidf_words$word[order(desc(tfidf_words$tf_idf))])
```

```{r}
ggplot(tfidf_words, aes(x = word, y = tf_idf))+
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

# Word Frequency by Job Category

Word count for every job category would not be beneficial to the final report so we will begin to do text analysis on each job category and then on all of the categories combined. We are doing this to see if there are any skills that applicants should have experience with before applying. This could help them become more successful applicants.

## Data Scientist

First, we tokenized our corpus and generated a word count.
```{r}
ds_words <- ds_jobs%>%
  filter(job_category == "Data Scientist")

job_words <- ds_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

Second, we removed stop words and generated a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

Lastly, we created a visualization of the word count distribution. 
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Data Scientist Category")
```

### Interpretation

After filtering by "Data Scientist," it looks like employees should have some kind of analytical skills as well as machine learning skills. "Models" is also in the top 15 so this could augment the machine learning skill that data scientists need. 

## Data Analyst

Now we will check to see if there are any skills that are important for data analysts.

First, we tokenized our corpus and generated a word count.
```{r}
analyst_words <- ds_jobs%>%
  filter(job_category == "Data Analyst")

job_words <- analyst_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

Second, we removed stop words and generated a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

Lastyl, we created a visualization of the word count distribution.
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Data Analyst Category")
```

### Interpretation

We don't learn as much from this graph as we did for the data scientist graph. However, we see soft skills such as "team", "analysis", "research" so the employee should be equipped with the ability to work with others and look at previous data to draw conclusions.

## Data Engineer

We did the same analysis for Data Engineer.

First, we tokenized our corpus and generated a word count.
```{r}
engineer_words <- ds_jobs%>%
  filter(job_category == "Data Engineer")

job_words <- engineer_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

Second, we removed stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

Lastly, we created a visualization of the word count distribution.
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Data Engineer Category")
```

### Interpretation

Python is listed as a commonly used word for data engineering so a data engineer ought to learn Python if they wish to be a successful applicant. Cloud is also a commonly used word so I assume this would be some type of google storage.

## Machine Learning Engineer

First we tokenized our corpus and generated a word count.
```{r}
ml_words <- ds_jobs%>%
  filter(job_category == "Machine Learning Engineer")

job_words <- ml_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

Secondly, we removed stop words and generated a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

Lastly, we created a visualization of the word count distribution.
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Machine Learning Engineer\nCategory")
```

### Interpretation

No hard skills are commonly used in the descriptions. Commonly used words such as "team" and "decisions" could suggest that a machine learning engineer must work with other to make decisions. This makes sense because the machine learning engineer will be making the predictive models for the company.

## Statisticians

Again, we did the same anaylsis on statistics job descriptions.

First, we tokenized our corpus and generated a word count.
```{r}
stats_words <- ds_jobs%>%
  filter(job_category == "Statistician")

job_words <- stats_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

Secondly, we removed stop words and generated a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

Lastly, we created a visualization of the word count distribution.
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Statistician Category")
```

### Interpretation

No hard skills in the job descriptions. It would be difficult to determine much from commonly used words in the job description for statisticians. 

## Other Analyst

Finally, we did the same analysis for Other Analyst. 

First, we tokenized our corpus and generated a word count.
```{r}
other_analyst_words <- ds_jobs%>%
  filter(job_category == "Other Analyst")

job_words <- other_analyst_words %>% select(job_category,job_desc) %>% 
             unnest_tokens(word, job_desc)
head(job_words)
```

Secondly, we removed stop words and generated a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

Lastly, we created a visualization of the word count distribution.
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words in the Other Analyst Category")
```

### Interpretation

Nothing to learn from this analysis. Only 28 jobs are in this category and it is grouped at other so it is hard to perform analysis on this group. There are soft skills that were seen in other data science positions such as analytical skills.

# All DS jobs

After looking at data science role separately, we wanted to look at job descriptions for data science roles as a whole. 

```{r}
#filtering the data set for ds roles
ds_filter <- ds_jobs %>%
  filter(!is.na(job_category)) %>%
  filter(job_category == "Data Analyst" | job_category == "Data Engineer" | 
         job_category == "Data Scientist" | job_category == "Machine Learning Engineer" |
         job_category == "Other Analyst" | job_category == "Statistician")

job_words <- ds_filter %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
```

```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

```{r}
one_word <- better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words for all of the\nData Science Related Positions")

one_word
```

## Interpretation

This graph is all inclusive for the data science related positions. It shows that applicants who are interested in a data science related position should have an analytical mindset. They also should be able to work well in teams.

# Two Word Text Analysis

Let's see if we can get better results using two-word phrases for our text analysis rather than looking at each word seperately as we did above.

Tokenizing the text
```{r}
job_words <- ds_filter %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc,token = "ngrams",format = "text", n =2)
```

Removing stop words
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
head(better_line_words[,2])

# split words b/c two word phrases aren't in stop words
best <- separate(better_line_words,col = word, into = c("first", "second"), sep = " ")

# now take out stop words from the two created columns
best_line_words <- best %>% anti_join(stop_words, by = c("first"="word"))
better_line_words <- best_line_words %>% anti_join(stop_words, by = c("second"="word"))

# join the columns back together
better_line_words$word <- paste(better_line_words$first,better_line_words$second, sep = " ")
```

Creating a visualization
```{r}
two_word <- better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(
    x = "Words",
    title = "Frequency of Words for all of the\nData Science Related Positions\n(2 words)")

two_word
```

## Interpretation

We did not many interesting results by doing a text analysis with the the most used words so we are now doing a text analysis with the most commonly used two-word phrases. Now we can see popular jobs within our data set such as machine learning and data scientist.

# Combine Plots

```{r}
ggarrange(one_word,two_word,ncol = 2, nrow = 1)
```

# Conclusions

Going into this exploration, we were interested in finding out if there were a common words among job descriptions such as hard skills that applicants would need to learn (R, Python) that would help them get a data science related job. We found that this was not case. There were soft skills that applicants should look to hone before applying such as analytical skills and getting experience working in teams.
