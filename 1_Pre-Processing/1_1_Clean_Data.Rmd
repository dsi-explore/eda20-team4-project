---
title: "Loading and Cleaning the Data"
output: github_document
---

```{r, message = FALSE}
library(tidyverse)
library(ggplot2)
library(janitor)
```

# Read in Data

```{r}
NY <- read.csv("glassdoor_data/Data_Job_NY.csv")
SF <- read.csv("glassdoor_data/Data_Job_SF.csv")
TX <- read.csv("glassdoor_data/Data_Job_TX.csv")
WA <- read.csv("glassdoor_data/Data_Job_WA.csv")

NY["Region"] <- "NYC"
SF["Region"] <- "SF"
TX["Region"] <- "TX"
WA["Region"] <- "DC"

#combining data from the 4 regions
ds_jobs <- rbind(NY, SF, TX, WA)
```

# Basic Exploration

```{r}
dim(ds_jobs)
```
There are 3,324 rows and 13 variables in the data set.

```{r}
head(ds_jobs)
```

```{r}
names(ds_jobs)
```

```{r}
summary(ds_jobs)
```

The unit of analysis in the `ds_jobs` data set is a specific job opening within the field of data science at a company listed on glassdoor.

# Clean Data

## Missing Values

It looks like `Min_salary` and `Max_Salary` have missing values denoted with a `-1` value. We will replace them with NA. 

```{r}
#Replace -1 with NA
ds_jobs["Min_Salary"] <- na_if(ds_jobs["Min_Salary"], -1)
ds_jobs["Max_Salary"] <- na_if(ds_jobs["Max_Salary"], -1)
```

```{r}
table(ds_jobs$State)
```

There are 2 observations within `State` that are blank. We will replace them with NA.

```{r}
#Replace blanks with NA
ds_jobs["State"] <- na_if(ds_jobs["State"], "")
```

```{r}
table(ds_jobs$Industry)
```

There are 624 observations with no `Industry`. We will replace them with NA.

```{r}
#Replace blanks with NA
ds_jobs["Industry"] <- na_if(ds_jobs["Industry"], "")
```

```{r}
table(ds_jobs$City)[1:10]
```

There are 6 observations with no `City`. We will replace them with NA. 

```{r}
#Replace blanks with NA
ds_jobs["City"] <- na_if(ds_jobs["City"], "")
```


## Data Types

```{r}
sapply(ds_jobs, class)
```

It looks like all the data types are correct except for `Date_Posted` and `Valid_until` which should be dates.

```{r}
#changing variables to date format
ds_jobs$Date_Posted <- as.Date(ds_jobs$Date_Posted, format = "%Y-%m-%d")  
ds_jobs$Valid_until <- as.Date(ds_jobs$Valid_until, format = "%Y-%m-%d")  
```

## Incorrect Values

```{r}
unique(ds_jobs$State)
```

There are some values in which the state value is `Texas` instead of `TX` and we need to be consistent with abbreviated states throughout the data set. 

```{r}
ds_jobs <- ds_jobs %>% mutate(State = case_when(
                              State == "Texas" ~ "TX",
                              T ~ State
))
```

There are some misspelled values for `City` that we want to clean up.

```{r}
#cleaning some city names
ds_jobs <- ds_jobs %>% mutate(City = case_when(
                              City == "Mc Lean" ~ "McLean",
                              City == "Crystal City, state=Virginia, Virginia" ~ "Crystal City",
                              T ~ City
))

#made alteration to add state to crystal city that was altered above
ds_jobs <- ds_jobs %>% 
  mutate(State = ifelse(City == 'Crystal City' & is.na(State), 'VA', State))
```

# Data Wrangling

## Metro Area

To make sense of the location of each job, we are grouping specific cities into metro areas.

```{r}
#create data frame of cities and their metro areas
City <- c("College Station", "Washington", "Dallas", "San Antonio","Houston", "Austin","San Francisco","New York", "Brooklyn", "Staten Island", "Bronx", "Maspeth", "Rego Park", "Lynbrook", "Mamaroneck", "Williston Park", "Fort Lee", "Jersey City", "Paramus", "West New York", "West Orange", "Addison", "Allen", "Angleton", "Arlington, TX", "Carrollton", "Denton", "Forney", "Fort Sam Houston", "Fort Worth", "Freeport", "Frisco", "Galveston", "Grapevine", "Harlingen", "Irving", "Kemah", "Kyle", "Leander", "Lewisville", "Pharr", "Plano", "Randolph A F B", "Richardson", "Round Rock", "Spring", "The Woodlands", "University Park", "Webster", "Weslaco", "Westlake", "Alameda", "Belmont","Berkeley", "Brisbane", "Burlingame", "Dublin", "Emeryville", "Fremont", "Hayward", "Hercules", "Menlo Park", "Novato", "Oakland", "Palo Alto", "Redwood City", "San Bruno", "San Carlos", "San Leandro", "San Mateo", "San Rafael", "San Ramon", "Sausalito", "South San Francisco", "Stanford", "Union City", "Woodside", "Annandale", "Arlington, VA", "Burke", "Centreville", "Chantilly", "Crystal City", "Fairfax", "Falls Church", "Fort Belvoir", "Herndon", "McLean", "Reston", "Rosslyn", "Sterling", "Tysons", "Vienna", "Adelphi", "Andrews AFB", "Annapolis Junction", "Beltsville", "Bethesda", "Bowie", "Chevy Chase", "College Park", "Fort Meade", "Fulton", "Gaithersburg", "Germantown", "Greenbelt", "Lanham", "North Bethesda", "Rockville", "Silver Spring", "Suitland", "White Oak", "Foster City", "Concord", "Richmond", "Albany", "Corpus Christi", "Alexandria", "Marshall", "Greenville", "Roanoke", "El Paso", "Lubbock", "Huntsville", "Italy", "Belton", "Comanche Village I", "Bryan", "Burnet", "Germany", "Springfield", "Columbia", "Laurel") 

metro_area <- c("College Station","Washington", "Dallas", "San Antonio", "Houston", "Austin","San Francisco","New York", "New York", "New York","New York", "New York","New York", "New York","New York", "New York","New York", "New York","New York", "New York", "New York", "Dallas", "Dallas", "Houston", "Dallas", "Dallas", "Dallas", "Dallas","San Antonio", "Dallas", "Houston", "Dallas", "Houston", "Dallas", "Brownsville", "Dallas", "Houston", "Austin", "Austin", "Dallas", "Brownsville", "Dallas", "San Antonio", "Dallas", "Austin", "Houston", "Houston", "Dallas", "Houston", "Brownsville", "Dallas", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "San Francisco", "San Francisco", "San Francisco","San Francisco", "Corpus Christi", "Washington", "Marshall", "Dallas", "Dallas", "El Paso", "Lubbock", "Huntsville", "Dallas", "Killeen", "Killeen", "College Station", "Killeen", "Lufkin", "Washington", "Washington", "Washington")

metro_df <- data.frame(City,metro_area)

#add the metro area to the data set
ds_jobs <- left_join(ds_jobs, metro_df, by = "City")
```

There are two Arlington's in the data set so we need to clean up these two metro areas based on state.

```{r}
ds_jobs <- ds_jobs  %>% mutate(metro_area = case_when(
                    City == "Arlington" & State == "TX" ~ "Dallas",
                    City == "Arlington" & State == "VA" ~ "Washington",
                    T ~ metro_area
))
```

There are still a few missing values for metro area.

```{r}
y <- ds_jobs[is.na(ds_jobs$metro_area),]

y %>% select(City, State, metro_area)
```

The following values remain:

* Florence, KY
* Chennai, TN
* Paris, TX
* Raleigh, NC
* 6 NAs

`KY`, `TN` and `NC` are not close to the other regions of the data and `Paris, TX` was not near any metro areas in the cost of living data set. We will remove these values from further analysis based on metro areas.

This chunk creates a csv with locations of cities and distance from state group centers.
```{r, message = FALSE}
#Find unique cities
city_counts <- ds_jobs %>% 
  mutate(Location = paste0(City, ', ', State),
         metro_location = case_when(metro_area == 'Washington' ~ 'Washington, DC',
                                    metro_area == 'New York' ~ 'New York, NY',
                                    T ~ paste0(metro_area, ', ', State))) %>% 
  group_by(City, State, Location, metro_area, metro_location) %>% 
  summarise(count = n()) %>% arrange(-count)

#Remove observations with no city (there are only 6 such postings)
city_counts <- city_counts[!is.na(city_counts$metro_area),]
city_counts <- city_counts[,-4]

########################################################
#load package to generate locations for each city
library(ggmap)
#register_google('redacted')
locations_city <- mutate_geocode(city_counts, Location)
locations_metro <- mutate_geocode(city_counts, metro_location)
```

```{r}
#observe counts for each metro location
locations_city %>%
  group_by(metro_location) %>%
  summarize(count = n()) %>%
  arrange(-count)
```

```{r, message = FALSE}
# bind locations of each city with their respective metro area
city_locations <- cbind(locations_city, locations_metro[, c(6,7)])

# rename location columns
names(city_locations)[6:9] <- c('lon_city', 'lat_city', 'lon_metro', 'lat_metro')

# calculate the proportion of a metro area's jobs located in each individual city, used for weighted metro location
job_concentrations <- city_locations %>% 
  group_by(metro_location) %>% 
  summarise(count, 
            job_concentration = count/sum(count))

# join job concentration data and city data
city_data <- city_locations %>% left_join(job_concentrations, by = c('metro_location', 'count')) %>% unique()

# calculate weighted locations of each metro area based on job concentrations and locations of cities
weighted_locations <- city_data %>% 
  group_by(metro_location) %>% 
  summarise(avg_lon_metro_weighted = sum(job_concentration*lon_city),
            avg_lat_metro_weighted = sum(job_concentration*lat_city))

# join weighted data to city data
city_data <- city_data %>% left_join(weighted_locations, by = 'metro_location')
```

```{r}
# check differences in regular and weighted metro locations, difference is minimal
city_data %>% 
  mutate(lat_diff = abs(lat_metro - avg_lat_metro_weighted),
         lon_diff = abs(lon_metro - avg_lon_metro_weighted)) %>% 
  select(metro_location, lat_diff, lon_diff) %>% 
  group_by(metro_location) %>% 
  summarise(lat_diff=max(lat_diff),
            lon_diff=max(lon_diff))

#create new column to determine distance from metro location and weighted metro location for each city
library(geosphere)
city_data <- city_data %>% 
  mutate(reg_dist = distm(c(lon_city, lat_city), c(lon_metro, lat_metro), fun = distHaversine)/1609,
         weight_dist = distm(c(lon_city, lat_city), 
                             c(avg_lon_metro_weighted, avg_lat_metro_weighted), fun = distHaversine)/1609)

# filter out small sample size metro areas and cities that are more than 50 miles away from their associated metro area's weighted center
city_data <- city_data %>% 
  filter(!metro_location %in% c('Brownsville, TX', 
                                'College Station, TX', 
                                'Corpus Christi, TX',
                                'El Paso, TX',
                                'Huntsville, TX',
                                'Killeen, TX',
                                'Lubbock, TX',
                                'Lufkin, TX',
                                'Marshall, TX'),
         weight_dist <= 50)

# filter ds jobs to only contain observations in metro areas that were established by this analysis
ds_jobs <- ds_jobs %>% 
  filter(paste0(City, ', ', State) %in% unique(city_data$Location)) %>% 
  left_join(city_data, by = c('City', 'State')) %>% 
  select(-Region, -metro_area)
```
 
From the above, we see that California, DC, and New York all seem to be reliable metro areas, all with less than 30 miles maximum from the average location. Texas on the other hand does not seem to be represented by a single metro area. Further inspection of the cities in Texas indicates several large concentrated cities that are not close to each other. We remove metro areas with small sample sizes and cities which were greater than 50 miles outside of their associated metro area. We end with Austin, Dallas, Houston, and San Antonio for Texas; New York, NY; San Francisco, CA; and Washington, DC as our metro areas of interest. 

## Cost of Living Index

Now we want to add the cost of living index for each observation based on metro area.

[Cost of Living Index](https://advisorsmith.com/data/coli/)

```{r}
#reading in the COI data
col_index <- read.csv("advisorsmith_cost_of_living_index.csv")

#create a city/state variable to join the tables
col_index$city_state <- paste0(col_index$City, ", ", col_index$State)

#adding the COI to the ds_jobs data
ds_jobs <- left_join(ds_jobs, col_index, by = c("metro_location" = "city_state"))
ds_jobs <- ds_jobs %>% select(-State.y, -City.y) %>% 
  select(COI = Cost.of.Living.Index, State = State.x, City = City.x, everything())
```

## Job Category

We want to create a `job_category` variable that will group different roles within data science. Currently there are only `job_title` and `job_desc` variables that are text. To identify patterns of data science roles and group similar jobs together, we performed text analysis on these two variables.

Here we are finding common data science words or roles within the `job_title` variable. 

```{r}
#From Job_title
data <- grep("data", ds_jobs$Job_title, ignore.case = TRUE, value = TRUE)
engineer <- grep("engineer", data, ignore.case = TRUE, value = TRUE)
analyst <- grep("anal", data, ignore.case = TRUE, value = TRUE)
ds <- grep("scien", data, ignore.case = TRUE, value = TRUE)
ml <- grep("machine", ds_jobs$Job_title, ignore.case = TRUE, value = TRUE)
stats <- grep("statistic", ds_jobs$Job_title, ignore.case = TRUE, value = TRUE)
model <- grep("model", ds_jobs$Job_title, ignore.case = TRUE, value = TRUE)
consult <- grep("consult", ds_jobs$Job_title, ignore.case = TRUE, value = TRUE)
bio <- grep("bio", ds_jobs$Job_title, ignore.case = TRUE, value = TRUE)
comp <- grep("computer scie", ds_jobs$Job_title, ignore.case = TRUE, value = TRUE)
other_analyst <- grep("analy", ds_jobs$Job_title, ignore.case = TRUE, value = TRUE)
research_scientist <- grep("research sci", ds_jobs$Job_title, ignore.case = TRUE, value = TRUE)
```

Here we are finding common data science words or roles within the `job_desc` variable.

```{r}
#From job_desc
data_desc <- grep("data", ds_jobs$Job_Desc, ignore.case = TRUE, value = TRUE)
engineer_desc <- grep("data engineer", ds_jobs$Job_Desc, ignore.case = TRUE, value = TRUE)
analyst_desc <- grep("data analy", ds_jobs$Job_Desc, ignore.case = TRUE, value = TRUE)
ds_desc <- grep("data scien", ds_jobs$Job_Desc, ignore.case = TRUE, value = TRUE)
ml_desc <- grep("machine", ds_jobs$Job_Desc, ignore.case = TRUE, value = TRUE)
stats_desc <- grep("statistic", ds_jobs$Job_Desc, ignore.case = TRUE, value = TRUE)
bio_desc <- grep("biology", ds_jobs$Job_Desc, ignore.case = TRUE, value = TRUE)
comp_desc <- grep("computer science", ds_jobs$Job_Desc, ignore.case = TRUE, value = TRUE)
```

Now we are creating a new variable `job_category` that groups job openings into similar roles by the above text analysis.

```{r}
#create new job_category variable
ds_jobs <- ds_jobs %>%
  mutate(job_category = NA)
  
#fill relevant values into the job_category variable
for (i in seq_along(ds_jobs$Job_title)) {
  if ((ds_jobs$Job_title[[i]] %in% engineer) && (is.na(ds_jobs$job_category[[i]]))) {
            ds_jobs$job_category[[i]] = "Data Engineer"
  } else if ((ds_jobs$Job_title[[i]] %in% analyst) &&
            (is.na(ds_jobs$job_category[[i]]))) {
            ds_jobs$job_category[[i]] = "Data Analyst"
  } else if ((ds_jobs$Job_title[[i]] %in% ds) && (is.na(ds_jobs$job_category[[i]]))) {
            ds_jobs$job_category[[i]] = "Data Scientist"
  } else if ((ds_jobs$Job_title[[i]] %in% ml) && (is.na(ds_jobs$job_category[[i]]))) {
            ds_jobs$job_category[[i]] = "Machine Learning Engineer"
  } else if ((ds_jobs$Job_title[[i]] %in% stats) && (is.na(ds_jobs$job_category[[i]]))){
            ds_jobs$job_category[[i]] = "Statistician"
  } else if((ds_jobs$Job_Desc[[i]] %in% engineer_desc) && 
            (is.na(ds_jobs$job_category[[i]]))) {
            ds_jobs$job_category[[i]] = "Data Engineer"
  } else if ((ds_jobs$Job_Desc[[i]] %in% analyst_desc) && 
             (is.na(ds_jobs$job_category[[i]]))) {
            ds_jobs$job_category[[i]] = "Data Analyst"
  } else if ((ds_jobs$Job_Desc[[i]] %in% ds_desc) &&
             (is.na(ds_jobs$job_category[[i]]))){
             ds_jobs$job_category[[i]] = "Data Scientist"
  } else if ((ds_jobs$Job_Desc[[i]] %in% ml_desc) && 
             (is.na(ds_jobs$job_category[[i]]))) {
             ds_jobs$job_category[[i]] = "Machine Learning Engineer"
  } else if ((ds_jobs$Job_Desc[[i]] %in% stats_desc) &&
             (is.na(ds_jobs$job_category[[i]]))) {
             ds_jobs$job_category[[i]] = "Statistician"
  } else if ((ds_jobs$Job_title[[i]] %in% consult) &&
             (is.na(ds_jobs$job_category[[i]]))) {
             ds_jobs$job_category[[i]] = "Consultant"
  } else if ((ds_jobs$Job_title[[i]] %in% bio) &&
             (is.na(ds_jobs$job_category[[i]]))) {
             ds_jobs$job_category[[i]] = "Biologist"
  } else if ((ds_jobs$Job_Desc[[i]] %in% bio_desc) &&
             (is.na(ds_jobs$job_category[[i]]))) {
             ds_jobs$job_category[[i]] = "Biologist"
  } else if ((ds_jobs$Job_title[[i]] %in% comp) && (is.na(ds_jobs$job_category[[i]]))) {
             ds_jobs$job_category[[i]] = "Computer Scientist"
  } else if ((ds_jobs$Job_Desc[[i]] %in% comp_desc) && 
             (is.na(ds_jobs$job_category[[i]]))) {
             ds_jobs$job_category[[i]] = "Computer Scientist"
  } else if ((ds_jobs$Job_title[[i]] %in% other_analyst) &&
             (is.na(ds_jobs$job_category[[i]]))) {
             ds_jobs$job_category[[i]] = "Other Analyst"
  } else if ((ds_jobs$Job_title[[i]] %in% model) && 
             (is.na(ds_jobs$job_category[[i]]))) {
             ds_jobs$job_category[[i]] = "Machine Learning Engineer"
  } else if ((ds_jobs$Job_title[[i]] %in% research_scientist) &&
             (is.na(ds_jobs$job_category[[i]]))) {
             ds_jobs$job_category[[i]] = "Research Scientist"
  }
}
```

## Scaled Salary

We want to create a new variable that scales the salary by the cost of living to be able to compare salaries between different metro areas.

```{r}
#creating the new variables
ds_jobs <- ds_jobs %>% mutate(min_scaled_salary = Min_Salary/(COI/100), max_scaled_salary = Max_Salary/(COI/100))
```


# Master Data set

Here we will save the final cleaned and merged data set to use for our analysis.

```{r}
#clean all the variable names for consistency
ds_jobs <- clean_names(ds_jobs)

write.csv(ds_jobs, "ds_jobs.csv", row.names = F)
```

# Time series analysis

This data about the job postings are listed from glassdoor scraped during covid time. According to the glassdoor findings, there were very few jobs posted around April end and there's a sudden rise the upcoming week. It's very clear that the job market was not stable during the pandemic.

```{R}
table_date_posted <- data.frame(table(ds_jobs$date_posted))
table_date_posted$Var1 <- as.Date(table_date_posted$Var1, format = "%Y-%m-%d") 
class(table_date_posted$Var1)
ggplot(table_date_posted, aes(x = Var1, y = Freq)) +
  geom_line() +
  labs(title = "Job availability vs Date posted",
       x = "Date",
       y = "Number of jobs")
```