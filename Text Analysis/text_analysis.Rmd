---
title: "Job Description Text Analysis"
output: github_document
editor_options: 
  chunk_output_type: console
---
<details>
  <summary>Click to expand!</summary>
# Libraries

```{r}
loadPkg = function(toLoad){
  for(lib in toLoad){
    if(! lib %in% installed.packages()[,1])
    { install.packages(lib, repos='http://cran.rstudio.com/') }
    suppressMessages( library(lib, character.only=TRUE) ) }
}
packs=c('tidyverse', 'tidytext', 'textdata')
loadPkg(packs)
library(readr)
setwd("C:/Users/Matt Flaherty/Documents/Projects/eda20-team4-project")
ds_jobs <- read_csv("Data Cleaning/ds_jobs.csv")
```
</details>
## Word count

I want to use text analysis on the job description to see if there is a skill that is repeated throughout. This would be an important skill if it were repeated that applicants should attempt to improve if they are interested in the jobs.

<details>
  <summary>Click to expand!</summary>
  
1. Tokenize your corpus and generate a word count.
```{r}
job_words <- ds_jobs %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)

job_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

```{r}
personal_stop_words <- stop_words %>% select(-lexicon) %>% 
  bind_rows(data.frame(word = c("â")))

better_line_words <- job_words %>% anti_join(personal_stop_words)
```
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

No skills were mentioned frequently so I will use TF-IDF just to see what the results yield. Keep in mind that this is for all of the jobs in the data set. I will filter by DS jobs later in the report.

# TF-IDF
<details>
  <summary>Click to expand</summary>
1. Generate a tf-idf measure of words in your dataset.
```{r}
idf_words <- ds_jobs %>% select(job_category, job_desc) %>% 
  unnest_tokens(word,job_desc) %>% count(job_category, word, sort = T)

better_idf_words <- idf_words %>% anti_join(personal_stop_words)

description_length <- better_idf_words %>% group_by(job_category) %>% summarize(total = sum(n()))

better_idf_words <- left_join(better_idf_words, description_length)

tfidf_words <- better_idf_words %>% bind_tf_idf(word, job_category, n)

tfidf_words <- tfidf_words %>% arrange(desc(tf_idf)) %>% slice(1:15)

tfidf_words %>% arrange(desc(tf_idf)) %>% head()
```

2. Create a visualization of the tf-idf measure and interpret your results.
```{r}
tfidf_words$word <- factor(tfidf_words$word, levels = tfidf_words$word[order(desc(tfidf_words$tf_idf))])
```
</details>
```{r}
ggplot(tfidf_words, aes(x = word, y = tf_idf))+
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

# Word Frequency by Job Category

## Data Scientist
<details>
  <summary>Click to expand</summary>
1. Tokenize your corpus and generate a word count.
```{r}
ds_words <- ds_jobs%>%
  filter(job_category == "Data Scientist")

job_words <- ds_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

```{r}
personal_stop_words <- stop_words %>% select(-lexicon) %>% 
  bind_rows(data.frame(word = c("â")))

better_line_words <- job_words %>% anti_join(personal_stop_words)
```
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

After filtering by "Data Scientist," it looks like employees should have some kind of analytical skills as well as machine learning skills. "Models" is also in the top 15 so this could augment the machine learning skill that data scientists need. 

## Data Analyst
<details>
  <summary>Click to expand!</summary>
1. Tokenize your corpus and generate a word count.
```{r}
analyst_words <- ds_jobs%>%
  filter(job_category == "Data Analyst")

job_words <- analyst_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

```{r}
personal_stop_words <- stop_words %>% select(-lexicon) %>% 
  bind_rows(data.frame(word = c("â")))

better_line_words <- job_words %>% anti_join(personal_stop_words)
```
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

We don't learn as much from this graph as we did for the data scientist graph. However, we see soft skills such as "team", "analysis", "research" so the employee should be equipped with the ability to work with others and look at previous data.

# Data Engineer

<details>
  <summary>Click to expand!</summary>
1. Tokenize your corpus and generate a word count.
```{r}
engineer_words <- ds_jobs%>%
  filter(job_category == "Data Engineer")

job_words <- engineer_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

## Interpretation

Python is listed as a commonly used word for data engineering. Cloud is also a commonly used word so I assume this would be some type of google storage.

# Machine Learning

<details>
  <summary>Click to expand!</summary>
1. Tokenize your corpus and generate a word count.
```{r}
ml_words <- ds_jobs%>%
  filter(job_category == "Machine Learning")

job_words <- ml_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

## Interpretation

No hard skills are commonly used in the descriptions. 

# Statistics

<details>
  <summary>Click to expand!</summary>
1. Tokenize your corpus and generate a word count.
```{r}
stats_words <- ds_jobs%>%
  filter(job_category == "Statistics")

job_words <- stats_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

## Interpretation

No hard skills in the job descriptions

# Other Analyst

<details>
  <summary>Click to expand!</summary>
1. Tokenize your corpus and generate a word count.
```{r}
other_analyst_words <- ds_jobs%>%
  filter(job_category == "Other Analyst")

job_words <- other_analyst_words %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
head(job_words)
```

2. Using the `TidyText` package, remove stop words and generate a new word count.
```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```

3. Create a visualization of the word count distribution and interpret your results.
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

## Interpretation

Nothing to learn from.

# All DS jobs
<details>
  <summary>Click to expand!</summary>
```{r}
ds_filter <- ds_jobs %>%
  filter(!is.na(job_category)) %>%
  filter(job_category == "Data Analyst" | job_category == "Data Engineer" | job_category == "Data Scientist" | job_category == "Machine Learning" | job_category == "Other Analyst" | job_category == "Statistics")

job_words <- ds_filter %>% select(job_category,job_desc) %>% unnest_tokens(word, job_desc)
```

```{r}
better_line_words <- job_words %>% anti_join(stop_words)
```
</details>
```{r}
better_line_words %>% count(word, sort = T) %>% slice(1:20) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

# Conclusions

Going into this exploration, I was interested in finding out if there was a common word among job descriptions such as a hard skill that applicants would need to learn (R, Python) that would help them get a data science related job. I found that this was not case. There were soft skills that applicants should look to hone before applying such as analytical skills.