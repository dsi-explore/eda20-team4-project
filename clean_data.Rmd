---
title: "Loading and Cleaning the Data"
output: github_document
---

```{r, message = FALSE}
library(tidyverse)
library(ggplot2)
```

# Read in Data

```{r}
NY <- read.csv("glassdoor_data/Data_Job_NY.csv")
SF <- read.csv("glassdoor_data/Data_Job_SF.csv")
TX <- read.csv("glassdoor_data/Data_Job_TX.csv")
WA <- read.csv("glassdoor_data/Data_Job_WA.csv")

NY["Region"] <- "NYC"
SF["Region"] <- "SF"
TX["Region"] <- "TX"
WA["Region"] <- "DC"

#combining data from the 4 regions
ds_jobs <- rbind(NY, SF, TX, WA)
```

# Basic Exploration

```{r}
dim(ds_jobs)
```
There are 3,324 rows and 12 variables in the data set.

```{r}
head(ds_jobs)
```
```{r}
names(ds_jobs)
```

```{r}
summary(ds_jobs)
```

The unit of analysis in the `ds_jobs` data set is a specific job within the field of data science at a company listed on glassdoor.

# Clean Data

## Missing Values

It looks like `Min_salary` and `Max_Salary` have missing values denoted with a `-1` value. We will replace them with NA. 

```{r}
#Replace -1 with NA
ds_jobs["Min_Salary"] <- na_if(ds_jobs["Min_Salary"], -1)
ds_jobs["Max_Salary"] <- na_if(ds_jobs["Max_Salary"], -1)
```

```{r}
table(ds_jobs$State)
```

There are 2 observations within `State` that are blank. We will replace them with NA.

```{r}
#Replace blanks with NA
ds_jobs["State"] <- na_if(ds_jobs["State"], "")
```

```{r}
table(ds_jobs$Industry)
```

There are 624 observations with no `Industry`. We will replace them with NA.

```{r}
#Replace blanks with NA
ds_jobs["Industry"] <- na_if(ds_jobs["Industry"], "")
```

```{r}
table(ds_jobs$City)[1:10]
```

There are 6 observations with no `City`. We will replace them with NA. 

```{r}
#Replace blanks with NA
ds_jobs["City"] <- na_if(ds_jobs["City"], "")
```


## Data Types

```{r}
sapply(ds_jobs, class)
```

It looks like all the data types are correct except for `Date_Posted` and `Valid_until` which should be dates.

```{r}
ds_jobs$Date_Posted <- as.Date(ds_jobs$Date_Posted, format = "%Y-%m-%d")  
ds_jobs$Valid_until <- as.Date(ds_jobs$Valid_until, format = "%Y-%m-%d")  
```

## Incorrect Values

```{r}
unique(ds_jobs$State)
```

There are some values in which the state value is `Texas` instead of `TX`.

```{r}
ds_jobs <- ds_jobs %>% mutate(State = case_when(
                              State == "Texas" ~ "TX",
                              T ~ State
))
```

There are some misspelled values for `City`.

```{r}
ds_jobs <- ds_jobs %>% mutate(City = case_when(
                              City == "Mc Lean" ~ "McLean",
                              City == "Crystal City, state=Virginia, Virginia" ~ "Crystal City",
                              T ~ City
))

#made alteration to add state to crystal city that was altered above
ds_jobs <- ds_jobs %>% 
  mutate(State = ifelse(City == 'Crystal City' & is.na(State), 'VA', State))
```

This chunk creates a csv with locations of cities and distance from state group centers
```{r}
#Find unique cities
city_counts <- ds_jobs %>% 
  mutate(Location = paste0(City, ', ', State)) %>% 
  group_by(City, State, Location) %>% 
  summarise(count = n()) %>% arrange(-count)

#Remove observations with no city (there are only 6 such postings)
city_counts <- city_counts[city_counts$Location != 'NA, TX',]

#load package to generate locations for each city
library(ggmap)
register_google('Your API Code')
locations_df <- mutate_geocode(city_counts, Location)

#join location data back to main city dataframe
city_locations <- city_counts %>% 
  left_join(locations_df[,-2], by = 'Location')

#remove observations that cannot possibly be part of our relevant metro areas
city_locations <- city_locations[!city_locations$State %in% c('TN', 'KY', 'NC'),]

#create groups for nearby states
city_locations <- city_locations %>% 
  mutate(state_group = case_when(
    State %in% 'CA' ~ 'CA',
    State %in% c('NY','NJ') ~ 'NY',
    State %in% c('DC','VA','MD') ~ 'DC',
    State %in% 'TX' ~ 'TX'
  )) 

#find weighted average longitude and latitude for each state group, weight determined by job concentration in given city
#weighted is more appropriate than regular average, but hardly changes overall numbers (tenths of a decimal)
group_con_loc <- city_locations %>% 
  group_by(state_group) %>% 
  summarise(count,
            job_concentration = count/sum(count),
            #avg_lon = mean(lon),
            #avg_lat = mean(lat),
            avg_lon_weighted = sum(job_concentration*lon),
            avg_lat_weighted = sum(job_concentration*lat)) %>% unique()

#join job concentration/average location data back into main city df
city_locations <- city_locations %>% left_join(group_con_loc, by = c('state_group', 'count'))

#create new column to determine distance from average for each city
library(geosphere)
city_locations <- city_locations %>% 
  mutate(dist = distm(c(lon, lat), c(avg_lon_weighted, avg_lat_weighted), fun = distHaversine)/1609)

#create csv for city job concentration and location data
#write.csv(city_locations, 'city_locations.csv')

```

Research state group city spreads
```{r}
city_locations <- read.csv('city_locations.csv')

city_locations %>% 
  group_by(state_group) %>% 
  summarise(max_dist = max(dist),
            min_dist = min(dist),
            avg_dist = mean(dist))
```

From the above, we see that California, DC, and New York all seem to be reliable metro areas, all with less than 30 miles maximum from the average location. Texas on the other hand does not seem to be represented by a single metro area. Further inspection of the cities in Texas indicates several large concentrated cities that are not close to each other. 

# Data Wrangling

##Metro Area

To make sense of the location of each job, we are grouping specific cities into metro areas.

```{r}
#create data frame of cities and their metro areas
City <- c("College Station", "Washington", "Dallas", "San Antonio","Houston", "Austin","San Francisco","New York", "Brooklyn", "Staten Island", "Bronx", "Maspeth", "Rego Park", "Lynbrook", "Mamaroneck", "Williston Park", "Fort Lee", "Jersey City", "Paramus", "West New York", "West Orange", "Addison", "Allen", "Angleton", "Arlington, TX", "Carrollton", "Denton", "Forney", "Fort Sam Houston", "Fort Worth", "Freeport", "Frisco", "Galveston", "Grapevine", "Harlingen", "Irving", "Kemah", "Kyle", "Leander", "Lewisville", "Pharr", "Plano", "Randolph A F B", "Richardson", "Round Rock", "Spring", "The Woodlands", "University Park", "Webster", "Weslaco", "Westlake", "Alameda", "Belmont","Berkeley", "Brisbane", "Burlingame", "Dublin", "Emeryville", "Fremont", "Hayward", "Hercules", "Menlo Park", "Novato", "Oakland", "Palo Alto", "Redwood City", "San Bruno", "San Carlos", "San Leandro", "San Mateo", "San Rafael", "San Ramon", "Sausalito", "South San Francisco", "Stanford", "Union City", "Woodside", "Annandale", "Arlington, VA", "Burke", "Centreville", "Chantilly", "Crystal City", "Fairfax", "Falls Church", "Fort Belvoir", "Herndon", "McLean", "Reston", "Rosslyn", "Sterling", "Tysons", "Vienna", "Adelphi", "Andrews AFB", "Annapolis Junction", "Beltsville", "Bethesda", "Bowie", "Chevy Chase", "College Park", "Fort Meade", "Fulton", "Gaithersburg", "Germantown", "Greenbelt", "Lanham", "North Bethesda", "Rockville", "Silver Spring", "Suitland", "White Oak", "Foster City", "Concord", "Richmond", "Albany", "Corpus Christi", "Alexandria", "Marshall", "Greenville", "Roanoke", "El Paso", "Lubbock", "Huntsville", "Italy", "Belton", "Comanche Village I", "Bryan", "Burnet", "Germany", "Springfield", "Columbia", "Laurel") 

metro_area <- c("College Station","Washington", "Dallas", "San Antonio", "Houston", "Austin","San Francisco","New York", "New York", "New York","New York", "New York","New York", "New York","New York", "New York","New York", "New York","New York", "New York", "New York", "Dallas", "Dallas", "Houston", "Dallas", "Dallas", "Dallas", "Dallas","San Antonio", "Dallas", "Houston", "Dallas", "Houston", "Dallas", "Brownsville", "Dallas", "Houston", "Austin", "Austin", "Dallas", "Brownsville", "Dallas", "San Antonio", "Dallas", "Austin", "Houston", "Houston", "Dallas", "Houston", "Brownsville", "Dallas", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "San Francisco", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "Washington", "San Francisco", "San Francisco", "San Francisco","San Francisco", "Corpus Christi", "Washington", "Marshall", "Dallas", "Dallas", "El Paso", "Lubbock", "Huntsville", "Dallas", "Killeen", "Killeen", "College Station", "Killeen", "Lufkin", "Washington", "Washington", "Washington")

metro_df <- data.frame(City,metro_area)

#add the metro area to the data set
ds_jobs <- left_join(ds_jobs, metro_df, by = "City")
```

There are two Arlington's in the data set so we need to clean up these two metro areas based on state.

```{r}
ds_jobs <- ds_jobs  %>% mutate(metro_area = case_when(
                    City == "Arlington" & State == "TX" ~ "Dallas",
                    City == "Arlington" & State == "VA" ~ "Washington",
                    T ~ metro_area
))
```

There are still a few missing values for metro area.

```{r}
y <- ds_jobs[is.na(ds_jobs$metro_area),]

y %>% select(City, State, metro_area)
```
The following values remain:

* Florence, KY
* Chennai, TN
* Paris, TX
* Raleigh, NC
* 6 NAs

`KY`, `TN` and `NC` are not close to the other regions of the data and `Paris, TX` was not near any metro areas in the cost of living data set. We might need to remove these values from the data set.

## Cost of Living Index

Now we want to add the cost of living index for each observation based on metro area.

[Cost of Living Index](https://advisorsmith.com/data/coli/)

```{r}
#reading in the COI data
col_index <- read.csv("advisorsmith_cost_of_living_index.csv")

#adding the COI to the ds_jobs data
ds_jobs <- left_join(ds_jobs, col_index, by = c("metro_area" = "City"))
ds_jobs <- ds_jobs %>% select(-State.y) %>% select(COI = Cost.of.Living.Index, State = State.x, everything())
```


